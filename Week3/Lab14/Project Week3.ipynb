{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a6205ac",
   "metadata": {},
   "source": [
    "# GIKI DNN BOOTCAMP WEEK 3 PROJECT\n",
    "#### By : Muhammad Saad Amjad Khan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a06d23",
   "metadata": {},
   "source": [
    "# Applying Filters on Images & Videos Using YOLOv3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab175cc",
   "metadata": {},
   "source": [
    "## YOLO: You Only Look Once\n",
    "\n",
    "### What is YOLO?\n",
    "\n",
    "YOLO (You Only Look Once) is a popular real-time object detection algorithm. It can detect multiple objects in images or videos and draw bounding boxes around them. YOLO is known for its speed and accuracy, making it suitable for real-time applications.\n",
    "\n",
    "### How Does YOLO Work?\n",
    "\n",
    "YOLO treats object detection as a single regression problem, straight from image pixels to bounding box coordinates and class probabilities. Here's a simplified explanation:\n",
    "\n",
    "1. **Input Image**: The input image is divided into a grid of S x S cells.\n",
    "2. **Bounding Boxes**: Each grid cell predicts B bounding boxes and their confidence scores.\n",
    "3. **Class Probabilities**: Each grid cell also predicts class probabilities for the object.\n",
    "4. **Combining Predictions**: YOLO combines these predictions to produce final bounding boxes with associated class labels.\n",
    "\n",
    "### Advantages of YOLO\n",
    "\n",
    "- **Speed**: YOLO is incredibly fast because it makes predictions with a single network pass.\n",
    "- **Accuracy**: It achieves high accuracy by considering contextual information in predictions.\n",
    "- **Simplicity**: YOLO's design is simple, making it easy to understand and implement.\n",
    "\n",
    "### Different Versions of YOLO\n",
    "\n",
    "Over the years, YOLO has evolved through several versions, each improving upon the last.\n",
    "\n",
    "#### YOLOv1\n",
    "\n",
    "- **Introduction**: The first version of YOLO, introduced in 2016.\n",
    "- **Architecture**: Uses a single convolutional neural network (CNN) to predict bounding boxes and class probabilities directly.\n",
    "- **Speed**: Achieves real-time processing speeds.\n",
    "\n",
    "#### YOLOv2 (YOLO9000)\n",
    "\n",
    "- **Introduction**: Released in 2017, also known as YOLO9000.\n",
    "- **Improvements**:\n",
    "  - **Batch Normalization**: Helps in stabilizing the training process and improving accuracy.\n",
    "  - **Anchor Boxes**: Introduces anchor boxes for better bounding box predictions.\n",
    "  - **Multi-Scale Training**: Trains the model at different scales to improve performance.\n",
    "- **Speed and Accuracy**: Better balance between speed and accuracy compared to YOLOv1.\n",
    "\n",
    "#### YOLOv3\n",
    "\n",
    "- **Introduction**: Released in 2018.\n",
    "- **Improvements**:\n",
    "  - **Darknet-53 Backbone**: Uses a deeper network (53 convolutional layers) for feature extraction.\n",
    "  - **Multi-Scale Predictions**: Makes predictions at three different scales to detect objects of various sizes.\n",
    "  - **Improved Bounding Box Predictions**: Enhances the accuracy of bounding box predictions.\n",
    "- **Performance**: Improved performance and accuracy over YOLOv2.\n",
    "\n",
    "#### YOLOv4\n",
    "\n",
    "- **Introduction**: Released in 2020.\n",
    "- **Improvements**:\n",
    "  - **CSPDarknet53 Backbone**: Uses Cross-Stage Partial connections for better feature extraction.\n",
    "  - **Bag of Freebies (BoF)**: Includes various training techniques that improve accuracy without increasing inference time.\n",
    "  - **Bag of Specials (BoS)**: Includes additional layers and modules that enhance performance.\n",
    "- **State-of-the-Art**: Achieves state-of-the-art performance in real-time object detection.\n",
    "\n",
    "#### YOLOv5\n",
    "\n",
    "- **Introduction**: Developed by Ultralytics and released in 2020.\n",
    "- **Improvements**:\n",
    "  - **PyTorch Implementation**: Implements YOLO in the PyTorch framework, making it more accessible.\n",
    "  - **Ease of Use**: Provides a user-friendly interface and pre-trained models for quick deployment.\n",
    "  - **Enhanced Features**: Includes various improvements in training techniques and model architecture.\n",
    "- **Popularity**: Widely adopted due to its ease of use and integration with PyTorch.\n",
    "\n",
    "#### YOLOv6 and Beyond\n",
    "\n",
    "- **Continued Evolution**: YOLO continues to evolve with new versions being developed, incorporating advancements in neural network architectures and training techniques.\n",
    "- **Focus Areas**: Emphasis on improving accuracy, speed, and robustness in real-world applications.\n",
    "\n",
    "### Applications of YOLO\n",
    "\n",
    "YOLO is used in a variety of applications due to its speed and accuracy:\n",
    "\n",
    "- **Autonomous Vehicles**: Detecting pedestrians, vehicles, and other objects in real-time.\n",
    "- **Surveillance**: Monitoring security cameras for suspicious activities.\n",
    "- **Robotics**: Enabling robots to perceive and interact with their environment.\n",
    "- **Healthcare**: Assisting in medical imaging analysis.\n",
    "- **Augmented Reality**: Enhancing real-world experiences with virtual objects.\n",
    "\n",
    "### Summary\n",
    "\n",
    "YOLO has revolutionized the field of object detection with its real-time capabilities and high accuracy. Each version of YOLO has brought significant improvements, making it a versatile and powerful tool for various applications. Understanding the different versions and their enhancements helps in selecting the right model for specific use cases.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01cdf32",
   "metadata": {},
   "source": [
    "## Filters: \n",
    "In computer vision, filters are algorithms or processes applied to images to enhance or modify their appearance, extract useful information, or identify specific features. Filters can be used for a variety of purposes, including noise reduction, edge detection, and image enhancement.\n",
    "\n",
    "Here are definitions of specific filters used iin this project:\n",
    "\n",
    "### 1. Detect Objects: \n",
    "Object detection involves identifying and locating objects within an image. This is typically done using machine learning algorithms like Convolutional Neural Networks (CNNs). Popular frameworks and models for object detection include YOLO (You Only Look Once), SSD (Single Shot MultiBox Detector), and Faster R-CNN\n",
    "\n",
    "### 2. Edge Detectio: \n",
    "Edge detection filters identify the boundaries within an image where there is a sharp change in intensity or color. Common edge detection algorithms include the Sobel, Canny, and Prewitt operators, which highlight the edges by calculating the gradient of the image intensity.\n",
    "\n",
    "### 3. Sharpen: \n",
    "A sharpen filter enhances the edges and fine details in an image, making it appear crisper. This is achieved by increasing the contrast between adjacent pixels. Unsharp masking is a popular technique used for sharpening.\n",
    "\n",
    "### 4. Blur: \n",
    "A blur filter softens an image by averaging the pixels around each pixel, reducing detail and noise. Common types include Gaussian blur, which uses a Gaussian function to create a smooth, blurred effect.\n",
    "\n",
    "### 5. Sepia: \n",
    "A sepia filter gives an image a warm brown tone, simulating the appearance of old photographs. This effect is achieved by altering the color values, typically by adjusting the red, green, and blue channels.\n",
    "\n",
    "### 6. Negative: \n",
    "A negative filter inverts the colors of an image, transforming each pixel to its complementary color. This is done by subtracting each color value from the maximum value (e.g., 255 for an 8-bit image).\n",
    "\n",
    "### 7. Cartoon: \n",
    "A cartoon filter simplifies an image by reducing the number of colors and enhancing edges, giving it a hand-drawn, cartoon-like appearance. This is often done by combining edge detection with bilateral filtering or other smoothing techniques.\n",
    "\n",
    "\n",
    "These filters and techniques are essential tools in computer vision, enabling a wide range of applications from image enhancement to complex tasks like object recognition and scene understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058cb24e",
   "metadata": {},
   "source": [
    "## Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import cv2\n",
    "import numpy as np\n",
    "import threading\n",
    "\n",
    "class YOLOFaceDetectionApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"YOLO Object Detection and Filters\")\n",
    "\n",
    "        self.weights_path = \"\"\n",
    "        self.cfg_path = \"\"\n",
    "        self.names_path = \"\"\n",
    "\n",
    "        self.panel = tk.Label(root)\n",
    "        self.panel.pack(padx=10, pady=10)\n",
    "\n",
    "        btn_frame = tk.Frame(root)\n",
    "        btn_frame.pack(fill=tk.X, pady=10)\n",
    "\n",
    "        btn_select_weights = tk.Button(btn_frame, text=\"Select Weights\", command=self.select_weights)\n",
    "        btn_select_weights.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        btn_select_cfg = tk.Button(btn_frame, text=\"Select CFG\", command=self.select_cfg)\n",
    "        btn_select_cfg.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        btn_select_names = tk.Button(btn_frame, text=\"Select Names\", command=self.select_names)\n",
    "        btn_select_names.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        btn_select_image = tk.Button(btn_frame, text=\"Select Image\", command=self.select_image)\n",
    "        btn_select_image.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        btn_select_video = tk.Button(btn_frame, text=\"Select Video\", command=self.select_video)\n",
    "        btn_select_video.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        btn_live_video = tk.Button(btn_frame, text=\"Live Video\", command=self.toggle_live_video)\n",
    "        btn_live_video.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        btn_detect_objects = tk.Button(btn_frame, text=\"Detect Objects\", command=self.toggle_detect_objects)\n",
    "        btn_detect_objects.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        btn_edge_detection = tk.Button(btn_frame, text=\"Edge Detection\", command=self.toggle_edge_detection)\n",
    "        btn_edge_detection.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        btn_sharpen = tk.Button(btn_frame, text=\"Sharpen\", command=self.toggle_sharpen)\n",
    "        btn_sharpen.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        btn_blur = tk.Button(btn_frame, text=\"Blur\", command=self.toggle_blur)\n",
    "        btn_blur.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        btn_sepia = tk.Button(btn_frame, text=\"Sepia\", command=self.toggle_sepia)\n",
    "        btn_sepia.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        btn_negative = tk.Button(btn_frame, text=\"Negative\", command=self.toggle_negative)\n",
    "        btn_negative.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        btn_cartoon = tk.Button(btn_frame, text=\"Cartoon\", command=self.toggle_cartoon)\n",
    "        btn_cartoon.pack(side=tk.LEFT, padx=10)\n",
    "\n",
    "        self.image_path = None\n",
    "        self.video_path = None\n",
    "        self.image = None\n",
    "        self.video_capture = None\n",
    "        self.net = None\n",
    "        self.classes = None\n",
    "        self.output_layers = None\n",
    "        self.filter_mode = None\n",
    "        self.detect_objects_flag = False\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "\n",
    "    def select_weights(self):\n",
    "        self.weights_path = filedialog.askopenfilename()\n",
    "        self.load_yolo()\n",
    "\n",
    "    def select_cfg(self):\n",
    "        self.cfg_path = filedialog.askopenfilename()\n",
    "        self.load_yolo()\n",
    "\n",
    "    def select_names(self):\n",
    "        self.names_path = filedialog.askopenfilename()\n",
    "        self.load_yolo()\n",
    "\n",
    "    def load_yolo(self):\n",
    "        if self.weights_path and self.cfg_path and self.names_path:\n",
    "            try:\n",
    "                self.net = cv2.dnn.readNet(self.weights_path, self.cfg_path)\n",
    "                self.layer_names = self.net.getLayerNames()\n",
    "                self.output_layers = [self.layer_names[i - 1] for i in self.net.getUnconnectedOutLayers()]\n",
    "                with open(self.names_path, \"r\") as f:\n",
    "                    self.classes = [line.strip() for line in f.readlines()]\n",
    "                messagebox.showinfo(\"YOLO\", \"YOLO model loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                messagebox.showerror(\"YOLO Error\", f\"Error loading YOLO: {e}\")\n",
    "\n",
    "    def select_image(self):\n",
    "        self.image_path = filedialog.askopenfilename()\n",
    "        if self.image_path:\n",
    "            self.load_image()\n",
    "\n",
    "    def select_video(self):\n",
    "        self.video_path = filedialog.askopenfilename()\n",
    "        if self.video_path:\n",
    "            if self.running:\n",
    "                self.stop_running()\n",
    "            else:\n",
    "                self.running = True\n",
    "                self.thread = threading.Thread(target=self.detect_objects_video)\n",
    "                self.thread.start()\n",
    "\n",
    "    def toggle_live_video(self):\n",
    "        if self.running:\n",
    "            self.stop_running()\n",
    "        else:\n",
    "            self.video_capture = cv2.VideoCapture(0)\n",
    "            self.running = True\n",
    "            self.thread = threading.Thread(target=self.show_live_video)\n",
    "            self.thread.start()\n",
    "\n",
    "    def load_image(self):\n",
    "        image = cv2.imread(self.image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        image = ImageTk.PhotoImage(image)\n",
    "\n",
    "        self.panel.config(image=image)\n",
    "        self.panel.image = image\n",
    "\n",
    "    def toggle_detect_objects(self):\n",
    "        self.detect_objects_flag = not self.detect_objects_flag\n",
    "        if self.image_path:\n",
    "            self.apply_filter_to_image()\n",
    "\n",
    "    def toggle_edge_detection(self):\n",
    "        self.filter_mode = \"edge\" if self.filter_mode != \"edge\" else None\n",
    "        if self.image_path:\n",
    "            self.apply_filter_to_image()\n",
    "\n",
    "    def toggle_sharpen(self):\n",
    "        self.filter_mode = \"sharpen\" if self.filter_mode != \"sharpen\" else None\n",
    "        if self.image_path:\n",
    "            self.apply_filter_to_image()\n",
    "\n",
    "    def toggle_blur(self):\n",
    "        self.filter_mode = \"blur\" if self.filter_mode != \"blur\" else None\n",
    "        if self.image_path:\n",
    "            self.apply_filter_to_image()\n",
    "\n",
    "    def toggle_sepia(self):\n",
    "        self.filter_mode = \"sepia\" if self.filter_mode != \"sepia\" else None\n",
    "        if self.image_path:\n",
    "            self.apply_filter_to_image()\n",
    "\n",
    "    def toggle_negative(self):\n",
    "        self.filter_mode = \"negative\" if self.filter_mode != \"negative\" else None\n",
    "        if self.image_path:\n",
    "            self.apply_filter_to_image()\n",
    "\n",
    "    def toggle_cartoon(self):\n",
    "        self.filter_mode = \"cartoon\" if self.filter_mode != \"cartoon\" else None\n",
    "        if self.image_path:\n",
    "            self.apply_filter_to_image()\n",
    "\n",
    "    def _detect_objects(self, frame):\n",
    "        return self.apply_yolo(frame)\n",
    "\n",
    "    def detect_objects_video(self):\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        while self.running:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if self.detect_objects_flag:\n",
    "                frame = self.apply_yolo(frame)\n",
    "            if self.filter_mode == \"edge\":\n",
    "                frame = cv2.Canny(frame, 100, 200)\n",
    "            elif self.filter_mode == \"sharpen\":\n",
    "                kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "                frame = cv2.filter2D(frame, -1, kernel)\n",
    "            elif self.filter_mode == \"blur\":\n",
    "                frame = cv2.GaussianBlur(frame, (15, 15), 0)\n",
    "            elif self.filter_mode == \"sepia\":\n",
    "                frame = self.apply_sepia(frame)\n",
    "            elif self.filter_mode == \"negative\":\n",
    "                frame = self.apply_negative(frame)\n",
    "            elif self.filter_mode == \"cartoon\":\n",
    "                frame = self.apply_cartoon(frame)\n",
    "            cv2.imshow(\"Video\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        self.running = False\n",
    "\n",
    "    def show_live_video(self):\n",
    "        while self.running:\n",
    "            ret, frame = self.video_capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if self.detect_objects_flag:\n",
    "                frame = self.apply_yolo(frame)\n",
    "            if self.filter_mode == \"edge\":\n",
    "                frame = cv2.Canny(frame, 100, 200)\n",
    "            elif self.filter_mode == \"sharpen\":\n",
    "                kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "                frame = cv2.filter2D(frame, -1, kernel)\n",
    "            elif self.filter_mode == \"blur\":\n",
    "                frame = cv2.GaussianBlur(frame, (15, 15), 0)\n",
    "            elif self.filter_mode == \"sepia\":\n",
    "                frame = self.apply_sepia(frame)\n",
    "            elif self.filter_mode == \"negative\":\n",
    "                frame = self.apply_negative(frame)\n",
    "            elif self.filter_mode == \"cartoon\":\n",
    "                frame = self.apply_cartoon(frame)\n",
    "            cv2.imshow(\"Live Video\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        self.stop_running()\n",
    "\n",
    "    def stop_running(self):\n",
    "        self.running = False\n",
    "        self.detect_objects_flag = False\n",
    "        self.filter_mode = None\n",
    "        if self.video_capture is not None:\n",
    "            self.video_capture.release()\n",
    "            self.video_capture = None\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def apply_yolo(self, image):\n",
    "        if not self.net:\n",
    "            messagebox.showerror(\"YOLO Error\", \"YOLO model is not loaded. Please load the model first.\")\n",
    "            return image\n",
    "\n",
    "        height, width, channels = image.shape\n",
    "        blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "        self.net.setInput(blob)\n",
    "        outs = self.net.forward(self.output_layers)\n",
    "\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    if (x, y, w, h) and isinstance(x, int) and isinstance(y, int) and isinstance(w, int) and isinstance(h, int):\n",
    "                        boxes.append([x, y, w, h])\n",
    "                        confidences.append(float(confidence))\n",
    "                        class_ids.append(class_id)\n",
    "\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indexes:\n",
    "                x, y, w, h = boxes[i]\n",
    "                label = str(self.classes[class_ids[i]])\n",
    "                confidence = confidences[i]\n",
    "                color = (0, 255, 0)\n",
    "                if isinstance(x, int) and isinstance(y, int) and isinstance(w, int) and isinstance(h, int):\n",
    "                    cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "                    cv2.putText(image, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "        return image\n",
    "\n",
    "    def apply_filter_to_image(self):\n",
    "        image = cv2.imread(self.image_path)\n",
    "        if self.detect_objects_flag:\n",
    "            image = self.apply_yolo(image)\n",
    "        if self.filter_mode == \"edge\":\n",
    "            image = cv2.Canny(image, 100, 200)\n",
    "        elif self.filter_mode == \"sharpen\":\n",
    "            kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
    "            image = cv2.filter2D(image, -1, kernel)\n",
    "        elif self.filter_mode == \"blur\":\n",
    "            image = cv2.GaussianBlur(image, (15, 15), 0)\n",
    "        elif self.filter_mode == \"sepia\":\n",
    "            image = self.apply_sepia(image)\n",
    "        elif self.filter_mode == \"negative\":\n",
    "            image = self.apply_negative(image)\n",
    "        elif self.filter_mode == \"cartoon\":\n",
    "            image = self.apply_cartoon(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(image)\n",
    "        image = ImageTk.PhotoImage(image)\n",
    "\n",
    "        self.panel.config(image=image)\n",
    "        self.panel.image = image\n",
    "\n",
    "    def apply_sepia(self, image):\n",
    "        sepia_filter = np.array([[0.272, 0.534, 0.131],\n",
    "                                 [0.349, 0.686, 0.168],\n",
    "                                 [0.393, 0.769, 0.189]])\n",
    "        image = cv2.transform(image, sepia_filter)\n",
    "        image = np.clip(image, 0, 255)\n",
    "        return image\n",
    "\n",
    "    def apply_negative(self, image):\n",
    "        return cv2.bitwise_not(image)\n",
    "\n",
    "    def apply_cartoon(self, image):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.medianBlur(gray, 5)\n",
    "        edges = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "                                      cv2.THRESH_BINARY, 9, 9)\n",
    "        color = cv2.bilateralFilter(image, 9, 300, 300)\n",
    "        cartoon = cv2.bitwise_and(color, color, mask=edges)\n",
    "        return cartoon\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = YOLOFaceDetectionApp(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdee345e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
